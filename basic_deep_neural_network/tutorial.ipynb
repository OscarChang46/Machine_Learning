{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of array a is: (2, 2)\n",
      "The type of elements in array a: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,2], [3,4]])\n",
    "print(\"The shape of array a is:\", a.shape)\n",
    "print(\"The type of elements in array a:\", a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.91910903 0.6421956  0.75371223 0.13931457]\n",
      " [0.08731955 0.78800206 0.32615094 0.54106782]\n",
      " [0.24023518 0.54542293 0.4005545  0.71519189]]\n",
      "[[0.83667994 0.58848114 0.29615456 0.28101769 0.70559724]\n",
      " [0.42259643 0.05731599 0.74702731 0.45231301 0.17577474]\n",
      " [0.049377   0.29247534 0.06679913 0.75115649 0.06377152]\n",
      " [0.43190832 0.36417241 0.15197153 0.54671034 0.44329304]]\n"
     ]
    }
   ],
   "source": [
    "# set random seed to guarantee the reproducibility of your code on different devices\n",
    "np.random.seed(77)\n",
    "a = np.random.random((3, 4))\n",
    "print(a)\n",
    "b = np.random.random((4, 5))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3533685  0.39482924 0.30466203 0.88714812 0.58607517]\n",
      " [0.37367972 0.6517246  0.34650263 0.90884031 0.55173689]\n",
      " [0.4947141  0.55631368 0.34061103 0.98832754 0.52045244]]\n"
     ]
    }
   ],
   "source": [
    "print(a @ b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Operator of Numpy.array\n",
    "+ sum (broadcast property)\n",
    "+ matrix product\n",
    "+ elementwise product (broadcast property)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Broadcasting\n",
    "The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. There are, however, cases where broadcasting is a bad idea because it leads to inefficient use of memory that slows computation.\n",
    "\n",
    "\n",
    "![Broadcasting](https://numpy.org/doc/stable/_images/broadcasting_2.png)\n",
    "\n",
    "In some cases, broadcasting stretches both arrays to form an output array larger than either of the initial arrays.\n",
    "![Example 2](https://numpy.org/doc/stable/_images/broadcasting_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 50])\n"
     ]
    }
   ],
   "source": [
    "# define a torch tensor\n",
    "# even the same random seed in different python package generates different values\n",
    "torch.manual_seed(77)\n",
    "a = torch.rand((1000,4))\n",
    "b = torch.rand((4, 10))\n",
    "c = torch.rand((10, 50))\n",
    "d = a @ b @ c\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert from `Numpy.array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.91910903 0.6421956  0.75371223 0.13931457]\n",
      " [0.08731955 0.78800206 0.32615094 0.54106782]\n",
      " [0.24023518 0.54542293 0.4005545  0.71519189]]\n",
      "<class 'numpy.ndarray'>\n",
      "float64\n",
      "\n",
      "\n",
      "tensor([[0.9191, 0.6422, 0.7537, 0.1393],\n",
      "        [0.0873, 0.7880, 0.3262, 0.5411],\n",
      "        [0.2402, 0.5454, 0.4006, 0.7152]], dtype=torch.float64)\n",
      "<class 'torch.Tensor'>\n",
      "torch.float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(77)\n",
    "a = np.random.random((3, 4))\n",
    "print(a)\n",
    "print(type(a))\n",
    "print(a.dtype)\n",
    "\n",
    "print('\\n')\n",
    "# The Torch.tensor has the similar attribute as Numpy.array called `dtype`.\n",
    "a = torch.tensor(a)\n",
    "print(a)\n",
    "print(type(a))\n",
    "print(a.dtype)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation\n",
    "A tensor can be created with `requires_grad=True` so that `torch.autograd` records operations on them for automatic differentiation.\n",
    "\n",
    "`torch.autograd` provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare Tensor s for which gradients should be computed with the `requires_grad=True` keyword. As of now, we only support autograd for floating point `Tensor` types ( half, float, double and bfloat16) and complex `Tensor` types (cfloat, cdouble).\n",
    "\n",
    "|Attribute|Description | \n",
    "|-|-|\n",
    "|`backward`|Computes the sum of gradients of given tensors with respect to graph leaves.|\n",
    "|`grad`|Computes and returns the sum of gradients of outputs with respect to the inputs.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2., -2.],\n",
       "        [ 2.,  2.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n",
    "out = x.pow(2).sum()\n",
    "out.backward()\n",
    "x.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch.nn\n",
    "`Torch.nn` module provides large numbers of operators which are the basic building blocks for graphs.\n",
    "### Linear Layers\n",
    "\n",
    "|Name| Explanation |\n",
    "|-|-|\n",
    "|[`nn.Identity`](https://pytorch.org/docs/stable/generated/torch.nn.Identity.html#torch.nn.Identity)|A placeholder identity operator that is argument-insensitive.|\n",
    "|[`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)|Applies a linear transformation to the incoming data: $y = x\\cdot A^T + b$|\n",
    "|[`nn.Bilinear`](https://pytorch.org/docs/stable/generated/torch.nn.Bilinear.html#torch.nn.Bilinear)|Applies a bilinear transformation to the incoming data: $y = x_1^TAx_2 + b$ |\n",
    "|[`nn.LazyLinear`](https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear)|A `torch.nn.Linear` module where *in_features* is inferred.|\n",
    "\n",
    "### Nonlinear Activations\n",
    "|Name| Explanation |\n",
    "|-|-|\n",
    "|[`nn.ELU`](https://pytorch.org/docs/stable/generated/torch.nn.ELU.html#torch.nn.ELU)|Applies the Exponential Linear Unit (ELU) function, element-wise, as described in the paper: [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](https://arxiv.org/abs/1511.07289).|\n",
    "|[`nn.LeakyReLU`](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU)|Applies the element-wise function: $LeakyReLU(X)=\\max{(0, x)} + negativeslope * \\min{(0, x)}$|\n",
    "|[`nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)|Applies the rectified linear unit function element-wise: $ReLU(x) = (x)^+ = \\max{(0, x)}$ |\n",
    "|[`nn.Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid)|Applies the element-wise function: $\\frac{1}{1  + e^{-x}}$ |\n",
    "### Normalization Layers\n",
    "|Name| Explanation |\n",
    "|-|-|\n",
    "|[`nn.BatchNorm1d`](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d)|Applies Batch Normalization over a 2D or 3D input as described in the paper Batch Normalization: [`Accelerating Deep Network Training by Reducing Internal Covariate Shift`](https://arxiv.org/abs/1502.03167) .|\n",
    "\n",
    "### Loss Functions\n",
    "|Name| Explanation |\n",
    "|-|-|\n",
    "|[`nn.L1Loss`](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss)|Creates a criterion that measures the mean absolute error (MAE) between each element in the input $x$ and target $y$.|\n",
    "|[`nn.MSELoss`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)|Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input $x$ and target $y$.|\n",
    "|[`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)|This criterion computes the cross entropy loss between input logits and target.|\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Optimizer\n",
    "`torch.optim` is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future.\n",
    "\n",
    "### How to use an optimizer\n",
    "To use `torch.optim` you have to construct an optimizer object that will hold the current state and will update the parameters based on the computed gradients.\n",
    "\n",
    "### Constructing it\n",
    "To construct an `Optimizer` you have to give it an iterable containing the parameters (all should be `Variable` s) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![optimization](https://cs231n.github.io/assets/nn3/opt2.gif)\n",
    "![optimization_methods](https://cs231n.github.io/assets/nn3/opt1.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "W = torch.tensor((3,4))\n",
    "f = W @ W\n",
    "\n",
    "optimizer = optim.SGD([W], lr = 0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your First Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((1000, 4))\n",
    "y = x.pow(2).sum(1)\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.W1 = torch.rand((4,5))\n",
    "        self.b1 = torch.rand((1000))\n",
    "        self.W2 = torch.rand(5, 10)\n",
    "        self.b2 = torch.rand((1000))\n",
    "    def forward(self, x):\n",
    "        x = self.W1 @ x + self.b1\n",
    "        x = self.W2 @ x  + self.b2\n",
    "        return x\n",
    "model = MyModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your First Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize a Deep Neural Network\n",
    "\n",
    "reference link: https://zhuanlan.zhihu.com/p/232348083\n",
    "\n",
    "running the following command in your terminal after switch to the `./log` folder.\n",
    "```\n",
    "tensorboard --logdir=./ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0000, 0.0000, 0.0000, 0.6836],\n",
      "          [0.0000, 0.3555, 0.4327, 0.1902],\n",
      "          [0.0000, 1.1358, 1.7973, 2.0034],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 2.1114, 0.0000, 0.3716],\n",
      "          [0.5349, 0.0000, 0.0000, 2.2730],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2119, 0.0000, 0.1004, 0.2430]],\n",
      "\n",
      "         [[0.2611, 0.0000, 0.0574, 0.0000],\n",
      "          [1.0215, 0.0000, 0.0000, 0.1554],\n",
      "          [0.9703, 1.9999, 0.4493, 0.0000],\n",
      "          [0.6199, 1.0380, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.6803, 0.1997, 0.1155, 0.1446],\n",
      "          [1.6370, 0.3171, 0.0000, 0.0000],\n",
      "          [1.2820, 0.8807, 0.9972, 0.0000],\n",
      "          [0.6175, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.2061, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4912, 1.5697, 0.5438, 0.4003],\n",
      "          [0.0000, 1.0927, 1.3977, 1.3004],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.2109, 0.4041],\n",
      "          [1.3927, 0.0000, 0.1822, 0.0000],\n",
      "          [0.2164, 0.0000, 0.5642, 1.4079],\n",
      "          [1.3098, 0.7517, 0.0000, 0.0000]],\n",
      "\n",
      "         [[2.1039, 0.7080, 1.3642, 0.0000],\n",
      "          [0.4261, 0.0000, 0.0000, 0.8515],\n",
      "          [0.0436, 0.0000, 0.0000, 0.3302],\n",
      "          [0.0000, 0.0074, 0.0953, 0.0000]],\n",
      "\n",
      "         [[0.0000, 2.2297, 0.0000, 0.0000],\n",
      "          [0.0000, 0.1768, 1.2822, 0.0000],\n",
      "          [0.4206, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.7981, 0.6890, 0.7333]],\n",
      "\n",
      "         [[0.9306, 0.9388, 0.4993, 0.0000],\n",
      "          [1.1364, 0.5334, 0.4538, 0.0000],\n",
      "          [1.3905, 0.0000, 0.7572, 0.5268],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000, 0.6117],\n",
      "          [1.1281, 2.1358, 0.0000, 0.0000],\n",
      "          [1.1265, 0.0000, 1.4588, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.2856]]]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class modelViz(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(modelViz, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 64, 3, 1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 10, 3, 1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x = F.relu(x)\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x = F.relu(x)\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "modelviz = modelViz()\n",
    "# 创建输入\n",
    "sampledata = torch.rand(1, 3, 4, 4)\n",
    "# 看看输出结果对不对\n",
    "out = modelviz(sampledata)\n",
    "# print(out)  # 测试有输出，网络没有问题\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. 来用tensorflow进行可视化\n",
    "with SummaryWriter(\"./log\", comment=\"sample_model_visualization\") as sw:\n",
    "    sw.add_graph(modelviz, sampledata)\n",
    "torch.save(modelviz, \"./log/modelviz.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
